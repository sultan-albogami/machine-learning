{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem One\n",
    "\n",
    "![](../util/img/problem-one-img-01.png)\n",
    "\n",
    "As can be seen in the table, Shape is a binary class: triangle and square, with 3 attributes: Color,\n",
    "Outline, and Dot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Calculate the entropy of $H$ of the training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy at a given node $t$: $$E(t) = - \\sum_j p(j|t) \\log_2 p(j|t)$$\n",
    "\n",
    "where $p(j|t)$ is the relative frequency of class $j$ at node $t$.\n",
    "\n",
    "In the training set there are 9 squares and 5 triangles; thus, the class probabilities are as follows:\n",
    "\n",
    "- $p(\\square) = \\frac{9}{14}$\n",
    "- $p(\\triangle) = \\frac{5}{14}$\n",
    "    \n",
    "And\n",
    "\n",
    "- Enropy: $$E(H) = - (\\frac{9}{14}) \\log_2(\\frac{9}{14}) - (\\frac{5}{14}) \\log_2(\\frac{5}{14}) = 0.940\\ bits$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Compute Gain(Color), Gain(Outline), and Gain(Dot) as defined in the lecture notes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain is defined as:\n",
    "\n",
    "$$GAIN_{split} = Entropy(p) - (\\sum_{i=1}^k \\frac{n_i}{n} Entropy(i))$$\n",
    "\n",
    "Parent Node $p$ is split into $k$ partitions; $n_i$ is number of records in partition $i$.\n",
    "\n",
    "\n",
    "1. Computing $Gain(Color)$:\n",
    "\n",
    "    - $P(red)    = \\frac{5}{14}.$ This is to say 5 of the 14 objects are red.\n",
    "    - $I(red)    = -(\\frac{3}{5}) \\log_2(\\frac{3}{5})-(\\frac{2}{5}) \\log_2(\\frac{2}{5}) = 0.971\\ bits.$ Out of the five red objects, 2 are triangles and 3 are squares. <br /><br />\n",
    "\n",
    "    - $P(green)  = \\frac{5}{14}$  \n",
    "    - $I(green)  = -(\\frac{2}{5}) \\log_2(\\frac{2}{5})-(\\frac{3}{5}) \\log_2(\\frac{3}{5}) = 0.971\\ bits$  <br /><br />\n",
    "\n",
    "    - $P(yellow) = \\frac{4}{14}$  \n",
    "    - $I(yellow) = -(\\frac{4}{4}) \\log_2(\\frac{4}{4})-(\\frac{0}{4}) \\log_2(\\frac{0}{4}) = 0.0\\ bits$  <br /><br />\n",
    "\n",
    "$$I_{res}(Color) = \\sum{p(v)I(v)} = \\frac{5}{14}(0.971) + \\frac{5}{14}(0.971) + \\frac{4}{14}(0.0) = 0.694\\ bits$$\n",
    "\n",
    "$$Gain(Color) = I - I_{res}(Color) = 0.940 - 0.694 = 0.246\\ bits$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Computing $Gain(Outline)$:\n",
    "\n",
    "    - $P(dashed) = \\frac{7}{14}$ \n",
    "    - $I(dashed) = -(\\frac{4}{7}) \\log_2(\\frac{4}{7})-(\\frac{3}{7}) \\log_2(\\frac{3}{7}) = 0.985\\ bits$  <br /><br />\n",
    "\n",
    "    - $P(solid)  = \\frac{7}{14}$  \n",
    "    - $I(solid)  = -(\\frac{6}{7}) \\log_2(\\frac{6}{7})-(\\frac{1}{7}) \\log_2(\\frac{1}{7}) = 0.592\\ bits$  <br /><br />\n",
    "\n",
    "$$I_{res}(Outline) = \\frac{7}{14}(0.985) + \\frac{7}{14}(0.592) = 0.789\\ bits$$\n",
    "\n",
    "$$Gain(Outline)    = 0.940 - 0.789 = 0.151\\ bits$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Computing $Gain(Dot)$:\n",
    "\n",
    "    - $P(yes) = \\frac{6}{14}$ \n",
    "    - $I(yes) = -(\\frac{3}{6}) \\log_2(\\frac{3}{6})-(\\frac{3}{6}) \\log_2(\\frac{3}{6}) =  1.0\\ bits$  <br /><br />\n",
    "\n",
    "    - $P(no)  = \\frac{8}{14}$  \n",
    "    - $I(no)  = -(\\frac{2}{8}) \\log_2(\\frac{2}{8})-(\\frac{6}{8}) \\log_2(\\frac{6}{8}) = 0.811\\ bits$  <br /><br />\n",
    "\n",
    "$$I_{res}(Dot) = \\frac{6}{14}(1.0) + \\frac{8}{14}(0.811) = 0.892\\ bits$$\n",
    "\n",
    "$$Gain(Dot)    = 0.940 - 0.892 = 0.048\\ bits$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Draw a decision tree based on the results obtained in part b).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute with the highest gain as shown in $b)$ is Color; hence, it is selected to be splitted on. After splitting on Color, there will be three nodes, namely yellow, green and red. For the yellow node, no further splitting is needed as $I(yellow) = 0.0\\ bits$ signifying that shape is determined. However, for the green and red nodes, further splitting is required to determine the shape of the objects.\n",
    "\n",
    "- Parent is green:\n",
    "    - Outline or Dot?\n",
    "        - When splitting on Outline: $Gain(Outline) = 0.971 - 0 = 0.971\\ bits.$\n",
    "        - When splitting on Dot: $Gain(Dot) = 0.971 - 0.951 = 0.020\\ bits.$\n",
    "\n",
    "\n",
    "- Parent is red:\n",
    "    - Outline or Dot?\n",
    "        - When splitting on Outline: $Gain(Outline) = 0.971 - 0.951 = 0.020\\ bits.$\n",
    "        - When splitting on Dot: $Gain(Dot) = 0.971 - 0 = 0.971\\ bits.$ <br /><br />\n",
    "\n",
    "Finally, to construct a decision tree classifying the shape of the objects in question, steps are as follows:\n",
    "\n",
    "- Split on Color\n",
    "- if the child node is green:\n",
    "    - split on Outline\n",
    "- else if the child node is red:\n",
    "    - split on Dot\n",
    "- else\n",
    "    - do nothing\n",
    "\n",
    "\n",
    "$$Decision \\ tree \\ classifying \\ the \\ shape.$$ \n",
    "\n",
    "\n",
    "![](../util/img/problem-one-img-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../util/img/problem-four-img.png-02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem Two:\n",
    "\n",
    "Consider the following data set for a binary class problem:\n",
    "\n",
    "![](../util/img/problem-two-img-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Calculate the information gain when splitting on A and B. Which\n",
    "attribute would the decision tree induction algorithm choose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data set the class probabilities are as follows:\n",
    "\n",
    "- $p(+) = \\frac{4}{10}$\n",
    "- $p(-) = \\frac{6}{10}$\n",
    "    \n",
    "And\n",
    "\n",
    "- Enropy: \n",
    "\n",
    "$$E(t) = -(\\frac{4}{10}) \\log_2(\\frac{4}{10}) - (\\frac{6}{10}) \\log_2(\\frac{6}{10}) = 0.971 \\ bits$$\n",
    "\n",
    "\n",
    "1. Computing $Gain(A)$:\n",
    "\n",
    "    - $p(T) = \\frac{7}{10}$ \n",
    "    - $I(T) = -(\\frac{4}{7}) \\log_2(\\frac{4}{7})-(\\frac{3}{7}) \\log_2(\\frac{3}{7}) = 0.9852\\ bits$  <br/><br/>\n",
    "\n",
    "    - $p(F) = \\frac{3}{10}$  \n",
    "    - $I(F) = -(\\frac{3}{3}) \\log_2(\\frac{3}{3})-(\\frac{0}{3}) \\log_2(\\frac{0}{3}) = 0\\ bits$  <br/><br/>\n",
    "\n",
    "$$I_{res}(A) = \\frac{7}{10}(0.9852) + \\frac{3}{10}(0) = 0.6896\\ bits$$\n",
    "\n",
    "$$Gain(A) = 0.9710 -  0.6896 = 0.2814\\ bits$$\n",
    "\n",
    "\n",
    "2. Computing $Gain(B)$:\n",
    "\n",
    "    - $p(T) = \\frac{4}{10}$ \n",
    "    - $I(T) = -(\\frac{3}{4}) \\log_2(\\frac{3}{4})-(\\frac{1}{4}) \\log_2(\\frac{1}{4}) = 0.8113\\ bits$  <br/><br/>\n",
    "\n",
    "    - $p(F) = \\frac{6}{10}$  \n",
    "    - $I(F) = -(\\frac{1}{6}) \\log_2(\\frac{1}{6})-(\\frac{5}{6}) \\log_2(\\frac{5}{6}) = 0.6500\\ bits$  <br/><br/>\n",
    "\n",
    "$$I_{res}(B) = \\frac{4}{10}(0.8113) + \\frac{6}{10}(0.6500) = 0.7145\\ bits$$\n",
    "\n",
    "$$Gain(B) = 0.9710 - 0.7145 = 0.2565\\ bits$$\n",
    "\n",
    "Attribute with the highest gain is A; thus, it is selected to be splitted on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Calculate the gain in the Gini index when splitting on A and B. Which\n",
    "attribute would the decision tree induction algorithm choose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$GINI(t) = \\sum_j[p(j|t)]^2$$ \n",
    "\n",
    "$$Gini(t) = 1 - (\\frac{4}{10})^2 - (\\frac{6}{10})^2 = 0.48$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Figure 4.13 shows that entropy and the Gini index are both monotonously\n",
    "increasing on the range [0, 0.5] and they are both monotonously decreasing\n",
    "on the range [0.5, 1]. Is it possible that information gain and the\n",
    "gain in the Gini index favor different attributes? Explain.**\n",
    "\n",
    "![](../util/img/problem-two-img-02.png)\n",
    "\n",
    "<h4><center>Figure 4.13 Comparison among the impurity measures for binary classification problems.</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Yes, although that they are monotonously increasing, thier behavior is different as they favor different attributes when splitting.</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h4><center>Works Cited<center><h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tan, Pang-Ning, et al. *Introduction to Data Mining.* Pearson, 2020.\n",
    "\n",
    "Khuri, Sami. \"Chapter 03: Modeling and Prediction\" Big Data and Machine Learning CSC410/610, 20 Feb. 2020, University of North Carolina at Greensboro. PDF presentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
